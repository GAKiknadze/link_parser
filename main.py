import asyncio
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urljoin, urlparse
import time
import signal
import sys
from dataclasses import dataclass
from typing import List, Dict, Optional, Set
from tqdm import tqdm
from tqdm.asyncio import tqdm as async_tqdm

@dataclass
class LinkInfo:
    text: str
    url: str
    status_code: Optional[int] = None
    is_valid: bool = False
    error: Optional[str] = None
    domain: str = ""
    response_time: float = 0.0

def setup_selenium_driver():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Selenium –¥—Ä–∞–π–≤–µ—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    options = Options()
    options.add_argument('--headless=new')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-infobars')
    options.add_argument('--disable-notifications')
    options.add_argument('--disable-popup-blocking')
    options.add_argument('--disable-logging')
    options.add_argument('--log-level=3')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--blink-settings=imagesEnabled=false')
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.managed_default_content_settings.video": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.geolocation": 2
    }
    options.add_experimental_option("prefs", prefs)
    
    # JavaScript –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
    options.add_argument('--disable-javascript')
    
    driver = webdriver.Chrome(options=options)
    
    # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ Selenium
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.navigator.chrome = {runtime: {}, app: {}};
            Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        '''
    })
    
    return driver

def get_links_with_selenium(url: str) -> List[LinkInfo]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –í–°–ï–• —Å—Å—ã–ª–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    driver = setup_selenium_driver()
    links = []
    seen_urls: Set[str] = set()
    
    try:
        print(f"\n{'='*60}")
        print(f"üåê –ó–ê–ì–†–£–ó–ö–ê –°–¢–†–ê–ù–ò–¶–´: {url}")
        print(f"{'='*60}")
        start_time = time.time()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        driver.set_page_load_timeout(20)
        driver.get(url)
        
        # –ñ–¥–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ DOM
        WebDriverWait(driver, 8).until(
            EC.presence_of_element_located((By.TAG_NAME, 'body'))
        )
        
        # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è SPA
        for _ in range(3):
            driver.execute_script("window.scrollBy(0, 500);")
            time.sleep(0.3)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
        a_tags = driver.find_elements(By.TAG_NAME, 'a')
        base_url = driver.current_url
        domain = urlparse(base_url).netloc
        
        print(f"\nüîó –°–ë–û–† –°–°–´–õ–û–ö –°–û –°–¢–†–ê–ù–ò–¶–´...")
        print(f"{'-'*60}")
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫
        for a in tqdm(a_tags, desc="–ü–∞—Ä—Å–∏–Ω–≥ —Å—Å—ã–ª–æ–∫", unit="—Å—Å—ã–ª–∫–∞", dynamic_ncols=True):
            try:
                href = a.get_attribute('href')
                text = a.text.strip()
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
                if not href or href.startswith(('javascript:', 'mailto:', 'tel:', 'file:', '#', 'about:', 'data:')):
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
                full_url = urljoin(base_url, href)
                parsed = urlparse(full_url)
                normalized_url = parsed._replace(
                    fragment="",
                    query=""
                ).geturl()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                if normalized_url in seen_urls or len(normalized_url) > 4096:
                    continue
                seen_urls.add(normalized_url)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                display_text = text[:100].replace('\n', ' ').replace('\r', ' ') + ("..." if len(text) > 100 else "")
                
                links.append(LinkInfo(
                    text=display_text if display_text else normalized_url[:50],
                    url=normalized_url,
                    domain=parsed.netloc or domain
                ))
            except Exception as e:
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        
        elapsed = time.time() - start_time
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed:.2f} —Å–µ–∫")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {str(e)}")
    finally:
        driver.quit()
    
    return links

async def check_url_status(session: aiohttp.ClientSession, url: str, timeout: float = 3.0) -> Dict:
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HEAD –∑–∞–ø—Ä–æ—Å–æ–≤"""
    start_time = time.time()
    result = {
        'url': url,
        'status_code': None,
        'is_valid': False,
        'error': None,
        'response_time': 0.0
    }
    
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º HEAD –∑–∞–ø—Ä–æ—Å
        try:
            async with session.head(
                url,
                allow_redirects=True,
                timeout=ClientTimeout(total=timeout, sock_read=1.5),
                headers={
                    'Connection': 'close',
                    'Accept': '*/*',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Cache-Control': 'no-cache',
                    'User-Agent': 'Mozilla/5.0 (compatible; LinkChecker/1.0; +https://example.com/bot)'
                },
                skip_auto_headers=['Accept-Encoding']
            ) as response:
                result['status_code'] = response.status
                result['is_valid'] = 200 <= response.status < 400
                result['response_time'] = time.time() - start_time
                return result
                
        except (aiohttp.ClientResponseError, aiohttp.ClientError) as e:
            # –ï—Å–ª–∏ HEAD –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (405), –ø—Ä–æ–±—É–µ–º GET —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
            if hasattr(e, 'status') and e.status == 405:
                pass
            else:
                raise
        
        # Fallback –∫ GET —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
        async with session.get(
            url,
            allow_redirects=True,
            timeout=ClientTimeout(total=timeout, sock_read=2.0),
            headers={
                'Connection': 'close',
                'Accept': 'text/html,application/xhtml+xml;q=0.9',
                'Accept-Language': 'en-US,en;q=0.9',
                'Cache-Control': 'no-cache',
                'Range': 'bytes=0-1023'  # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 1KB
            }
        ) as response:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–µ —á–∏—Ç–∞–µ–º —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞
            result['status_code'] = response.status
            result['is_valid'] = 200 <= response.status < 400
            result['response_time'] = time.time() - start_time
            return result
            
    except asyncio.TimeoutError:
        result['error'] = 'Timeout'
    except aiohttp.ClientResponseError as e:
        result['status_code'] = e.status
        result['is_valid'] = 200 <= e.status < 400
        result['error'] = str(e)
    except aiohttp.ClientError as e:
        result['error'] = f'Network: {str(e)}'
    except Exception as e:
        result['error'] = f'Unknown: {str(e)}'
    
    result['response_time'] = time.time() - start_time
    return result

async def check_links_ultra_fast(links: List[LinkInfo], max_connections: int = 200) -> List[LinkInfo]:
    """–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –í–°–ï–• —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    print(f"\n{'='*60}")
    print(f"‚ö° –ü–†–û–í–ï–†–ö–ê –°–¢–ê–¢–£–°–û–í {len(links)} –°–°–´–õ–û–ö")
    print(f"{'='*60}")
    print("–°—Ç—Ä–∞—Ç–µ–≥–∏—è: HEAD –∑–∞–ø—Ä–æ—Å—ã ‚Üí GET —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º ‚Üí —Ç–∞–π–º–∞—É—Ç—ã 3—Å")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {max_connections} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π, 20/–¥–æ–º–µ–Ω")
    print("-"*60)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–æ–º–µ–Ω–∞–º –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    domain_groups = {}
    for idx, link in enumerate(links):
        if link.domain not in domain_groups:
            domain_groups[link.domain] = []
        domain_groups[link.domain].append((idx, link))
    
    connector = TCPConnector(
        limit=max_connections,    # –û–±—â–∏–π –ª–∏–º–∏—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        limit_per_host=20,        # –õ–∏–º–∏—Ç –Ω–∞ –¥–æ–º–µ–Ω
        enable_cleanup_closed=True,
        force_close=True,
        ssl=False                 # –û—Ç–∫–ª—é—á–∞–µ–º SSL –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    )
    
    timeout = ClientTimeout(total=5.0, connect=2.0)
    
    async with aiohttp.ClientSession(
        connector=connector,
        timeout=timeout,
        headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Connection': 'close'
        },
        trust_env=False
    ) as session:
        
        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
        total_tasks = len(links)
        pbar = tqdm(total=total_tasks, desc="–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤", unit="—Å—Å—ã–ª–∫–∞", dynamic_ncols=True)
        
        async def process_domain(domain, items):
            results = []
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 20 —Å—Å—ã–ª–æ–∫ –∑–∞ —Ä–∞–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
            for i in range(0, len(items), 20):
                batch = items[i:i+20]
                tasks = [check_url_status(session, link.url) for _, link in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for (idx, link), res in zip(batch, batch_results):
                    if isinstance(res, Exception):
                        result = {'error': str(res)}
                    else:
                        result = res
                    
                    link.status_code = result.get('status_code')
                    link.is_valid = result.get('is_valid', False)
                    link.error = result.get('error')
                    link.response_time = result.get('response_time', 0.0)
                    results.append(link)
                    pbar.update(1)  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                
                # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
                await asyncio.sleep(0.02)
            return results
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        domain_tasks = [process_domain(domain, items) for domain, items in domain_groups.items()]
        
        try:
            domain_results = await async_tqdm.gather(*domain_tasks, desc="–î–æ–º–µ–Ω—ã", unit="–¥–æ–º–µ–Ω")
        except asyncio.CancelledError:
            print("\n\n‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            pbar.close()
            raise
        
        pbar.close()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        all_results = [None] * len(links)
        for domain_result in domain_results:
            for link in domain_result:
                # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Ö–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å
                for i, orig_link in enumerate(links):
                    if orig_link.url == link.url and all_results[i] is None:
                        all_results[i] = link
                        break
        
        return [link for link in all_results if link is not None]

def report_results(links: List[LinkInfo]):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    valid = [l for l in links if l.is_valid]
    invalid = [l for l in links if not l.is_valid]
    
    print(f"\n{'='*60}")
    print(f"‚úÖ –í–ê–õ–ò–î–ù–´–ï –°–°–´–õ–ö–ò (200-399): {len(valid)} –∏–∑ {len(links)}")
    print(f"{'-'*60}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-20 –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    for i, link in enumerate(valid[:20], 1):
        status_color = "\033[92m" if link.is_valid else "\033[91m"
        time_color = "\033[94m" if link.response_time < 1.0 else "\033[93m" if link.response_time < 2.0 else "\033[91m"
        reset = "\033[0m"
        
        print(f"{i}. {link.text[:70]}")
        print(f"   ‚Üí {link.url[:80]}")
        print(f"   üìä {status_color}{link.status_code}{reset} | ‚è±Ô∏è {time_color}{link.response_time:.3f}—Å{reset}\n")
    
    if len(valid) > 20:
        print(f"... –∏ –µ—â–µ {len(valid) - 20} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
    
    print(f"\n{'='*60}")
    print(f"‚ùå –ù–ï–í–ê–õ–ò–î–ù–´–ï –°–°–´–õ–ö–ò: {len(invalid)} –∏–∑ {len(links)}")
    print(f"{'-'*60}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-20 –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    for i, link in enumerate(invalid[:20], 1):
        status = link.status_code if link.status_code else "ERR"
        error = f" | {link.error[:50]}" if link.error else ""
        status_color = "\033[92m" if link.is_valid else "\033[91m"
        time_color = "\033[94m" if link.response_time < 1.0 else "\033[93m" if link.response_time < 2.0 else "\033[91m"
        reset = "\033[0m"
        
        print(f"{i}. {link.text[:70]}")
        print(f"   ‚Üí {link.url[:80]}")
        print(f"   üìä {status_color}{status}{reset}{error}")
        print(f"   ‚è±Ô∏è {time_color}{link.response_time:.3f}—Å{reset}\n")
    
    if len(invalid) > 20:
        print(f"... –∏ –µ—â–µ {len(invalid) - 20} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n{'='*60}")
    print("üìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*60}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å-–∫–æ–¥–∞–º
    status_counts = {}
    for link in links:
        status = link.status_code or "ERROR"
        status_counts[status] = status_counts.get(status, 0) + 1
    
    print("–°—Ç–∞—Ç—É—Å-–∫–æ–¥—ã:")
    for status, count in sorted(status_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(links)) * 100
        print(f"  {status}: {count} ({percentage:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
    domain_stats = {}
    for link in links:
        domain = link.domain or "unknown"
        if domain not in domain_stats:
            domain_stats[domain] = {'total': 0, 'valid': 0}
        domain_stats[domain]['total'] += 1
        if link.is_valid:
            domain_stats[domain]['valid'] += 1
    
    print(f"\n–¢–æ–ø-5 –¥–æ–º–µ–Ω–æ–≤:")
    top_domains = sorted(domain_stats.items(), key=lambda x: x[1]['total'], reverse=True)[:5]
    for domain, stats in top_domains:
        valid_percent = (stats['valid'] / stats['total']) * 100
        print(f"  {domain}: {stats['total']} —Å—Å—ã–ª–æ–∫ | –í–∞–ª–∏–¥–Ω—ã—Ö: {stats['valid']} ({valid_percent:.1f}%)")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n{'='*60}")
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print(f"{'='*60}")
    
    with open('valid_links.txt', 'w', encoding='utf-8') as f:
        for link in tqdm(valid, desc="–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∞–ª–∏–¥–Ω—ã—Ö", unit="—Å—Å—ã–ª–∫–∞"):
            f.write(f"{link.text} | {link.url} | {link.status_code} | {link.response_time:.3f}\n")
    
    with open('invalid_links.txt', 'w', encoding='utf-8') as f:
        for link in tqdm(invalid, desc="–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö", unit="—Å—Å—ã–ª–∫–∞"):
            status = link.status_code or "ERR"
            error = link.error or ""
            f.write(f"{link.text} | {link.url} | {status} | {error} | {link.response_time:.3f}\n")
    
    # JSON –æ—Ç—á–µ—Ç
    import json
    report = {
        'summary': {
            'total_links': len(links),
            'valid_links': len(valid),
            'invalid_links': len(invalid),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'avg_response_time': sum(l.response_time for l in links) / len(links) if links else 0
        },
        'links': [l.__dict__ for l in links]
    }
    
    with open('full_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"   ‚Ä¢ valid_links.txt ({len(valid)} —Å—Å—ã–ª–æ–∫)")
    print(f"   ‚Ä¢ invalid_links.txt ({len(invalid)} —Å—Å—ã–ª–æ–∫)")
    print(f"   ‚Ä¢ full_report.json (–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)")

async def main(url: str, max_connections: int = 200):
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –¥–ª—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤"""
    # –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–π
    def signal_handler(sig, frame):
        print("\n\n‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è. –ó–∞–≤–µ—Ä—à–∞–µ–º...")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    print("="*60)
    print("üöÄ –°–í–ï–†–•–ë–´–°–¢–†–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–°–´–õ–û–ö –° SELENIUM –ò –ü–†–û–ì–†–ï–°–°-–ë–ê–†–û–ú")
    print("="*60)
    print(f"üéØ –¶–µ–ª–µ–≤–æ–π URL: {url}")
    print(f"‚ö° –ú–∞–∫—Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {max_connections}")
    print(f"üìà –ù–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫")
    print("-"*60)
    
    total_start = time.time()
    
    # –≠—Ç–∞–ø 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ Selenium
    links = get_links_with_selenium(url)
    if not links:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        return
    
    # –≠—Ç–∞–ø 2: –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤
    check_start = time.time()
    
    try:
        checked_links = await check_links_ultra_fast(links, max_connections)
    except asyncio.CancelledError:
        print("‚ùó –ü—Ä–æ–≤–µ—Ä–∫–∞ –±—ã–ª–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞")
        return
    
    check_time = time.time() - check_start
    total_time = time.time() - total_start
    
    # –≠—Ç–∞–ø 3: –û—Ç—á–µ—Ç
    print(f"\n{'='*60}")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {check_time:.2f} —Å–µ–∫")
    print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {len(links)/check_time:.1f} —Å—Å—ã–ª–æ–∫/—Å–µ–∫")
    print(f"üèÅ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
    
    report_results(checked_links)
    
    print(f"\n{'='*60}")
    print(f"üéâ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print(f"{'='*60}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º')
    parser.add_argument('url', type=str, help='URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--connections', type=int, default=200, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 200)')
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è ChromeDriver
    try:
        from selenium import webdriver
    except ImportError:
        print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω selenium")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: pip install selenium aiohttp tqdm webdriver-manager")
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    asyncio.run(main(args.url, args.connections))