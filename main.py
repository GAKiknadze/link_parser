import asyncio
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urljoin, urlparse
import time
import signal
import sys
from dataclasses import dataclass
from typing import List, Dict, Optional, Set

@dataclass
class LinkInfo:
    text: str
    url: str
    status_code: Optional[int] = None
    is_valid: bool = False
    error: Optional[str] = None
    domain: str = ""
    response_time: float = 0.0

def setup_selenium_driver():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ Selenium –¥—Ä–∞–π–≤–µ—Ä–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    options = Options()
    options.add_argument('--headless=new')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-infobars')
    options.add_argument('--disable-notifications')
    options.add_argument('--disable-popup-blocking')
    options.add_argument('--disable-logging')
    options.add_argument('--log-level=3')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--blink-settings=imagesEnabled=false')
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option('useAutomationExtension', False)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –º–µ–¥–∏–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.managed_default_content_settings.video": 2,
        "profile.managed_default_content_settings.stylesheets": 2,
        "profile.default_content_setting_values.notifications": 2,
        "profile.default_content_setting_values.geolocation": 2
    }
    options.add_experimental_option("prefs", prefs)
    
    # JavaScript –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
    options.add_argument('--disable-javascript')
    
    driver = webdriver.Chrome(options=options)
    
    # –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ Selenium
    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
        'source': '''
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.navigator.chrome = {runtime: {}, app: {}};
            Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']});
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        '''
    })
    
    return driver

def get_links_with_selenium(url: str) -> List[LinkInfo]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    driver = setup_selenium_driver()
    links = []
    seen_urls: Set[str] = set()
    
    try:
        print(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        start_time = time.time()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        driver.set_page_load_timeout(15)
        driver.get(url)
        
        # –ñ–¥–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ DOM
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.TAG_NAME, 'body'))
        )
        
        # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.5)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
        a_tags = driver.find_elements(By.TAG_NAME, 'a')
        base_url = driver.current_url
        domain = urlparse(base_url).netloc
        
        for a in a_tags:
            try:
                href = a.get_attribute('href')
                text = a.text.strip()
                
                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
                if not href or href.startswith(('javascript:', 'mailto:', 'tel:', 'file:', 'data:', '#', 'about:')):
                    continue
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
                full_url = urljoin(base_url, href)
                parsed = urlparse(full_url)
                normalized_url = parsed._replace(
                    fragment="",
                    query=""
                ).geturl()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                if normalized_url in seen_urls or len(normalized_url) > 2048:
                    continue
                seen_urls.add(normalized_url)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                display_text = text[:100] + "..." if len(text) > 100 else text
                
                links.append(LinkInfo(
                    text=display_text if display_text else normalized_url[:50],
                    url=normalized_url,
                    domain=parsed.netloc or domain
                ))
            except:
                continue
        
        elapsed = time.time() - start_time
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –∑–∞ {elapsed:.2f} —Å–µ–∫")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫: {str(e)}")
    finally:
        driver.quit()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏
    return links[:200]

async def check_url_status(session: aiohttp.ClientSession, url: str, timeout: float = 2.0) -> Dict:
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HEAD –∑–∞–ø—Ä–æ—Å–æ–≤"""
    start_time = time.time()
    result = {
        'url': url,
        'status_code': None,
        'is_valid': False,
        'error': None,
        'response_time': 0.0
    }
    
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º HEAD –∑–∞–ø—Ä–æ—Å
        try:
            async with session.head(
                url,
                allow_redirects=True,
                timeout=ClientTimeout(total=timeout, sock_read=1.0),
                headers={
                    'Connection': 'close',
                    'Accept': '*/*',
                    'Accept-Language': 'en-US,en;q=0.9',
                    'Cache-Control': 'no-cache',
                    'User-Agent': 'Mozilla/5.0 (compatible; LinkChecker/1.0; +https://example.com/bot)'
                },
                skip_auto_headers=['Accept-Encoding']
            ) as response:
                result['status_code'] = response.status
                result['is_valid'] = 200 <= response.status < 400
                result['response_time'] = time.time() - start_time
                return result
                
        except (aiohttp.ClientResponseError, aiohttp.ClientError) as e:
            # –ï—Å–ª–∏ HEAD –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è (405), –ø—Ä–æ–±—É–µ–º GET —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
            if hasattr(e, 'status') and e.status == 405:
                pass
            else:
                raise
        
        # Fallback –∫ GET —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
        async with session.get(
            url,
            allow_redirects=True,
            timeout=ClientTimeout(total=timeout, sock_read=1.5),
            headers={
                'Connection': 'close',
                'Accept': 'text/html,application/xhtml+xml;q=0.9',
                'Accept-Language': 'en-US,en;q=0.9',
                'Cache-Control': 'no-cache',
                'Range': 'bytes=0-1023'  # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 1KB
            }
        ) as response:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–µ —á–∏—Ç–∞–µ–º —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞
            result['status_code'] = response.status
            result['is_valid'] = 200 <= response.status < 400
            result['response_time'] = time.time() - start_time
            return result
            
    except asyncio.TimeoutError:
        result['error'] = 'Timeout'
    except aiohttp.ClientResponseError as e:
        result['status_code'] = e.status
        result['is_valid'] = 200 <= e.status < 400
        result['error'] = str(e)
    except aiohttp.ClientError as e:
        result['error'] = f'Network: {str(e)}'
    except Exception as e:
        result['error'] = f'Unknown: {str(e)}'
    
    result['response_time'] = time.time() - start_time
    return result

async def check_links_ultra_fast(links: List[LinkInfo]) -> List[LinkInfo]:
    """–£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–æ–º–µ–Ω–∞–º –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
    domain_groups = {}
    for idx, link in enumerate(links):
        if link.domain not in domain_groups:
            domain_groups[link.domain] = []
        domain_groups[link.domain].append((idx, link))
    
    connector = TCPConnector(
        limit=100,           # –û–±—â–∏–π –ª–∏–º–∏—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        limit_per_host=15,   # –õ–∏–º–∏—Ç –Ω–∞ –¥–æ–º–µ–Ω
        enable_cleanup_closed=True,
        force_close=True,
        ssl=False            # –û—Ç–∫–ª—é—á–∞–µ–º SSL –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    )
    
    timeout = ClientTimeout(total=3.0, connect=1.0)
    
    async with aiohttp.ClientSession(
        connector=connector,
        timeout=timeout,
        headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': '*/*',
            'Connection': 'close'
        },
        trust_env=False
    ) as session:
        
        async def process_domain(domain, items):
            results = []
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 15 —Å—Å—ã–ª–æ–∫ –∑–∞ —Ä–∞–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
            for i in range(0, len(items), 15):
                batch = items[i:i+15]
                tasks = [check_url_status(session, link.url) for _, link in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for (idx, link), res in zip(batch, batch_results):
                    if isinstance(res, Exception):
                        result = {'error': str(res)}
                    else:
                        result = res
                    
                    link.status_code = result.get('status_code')
                    link.is_valid = result.get('is_valid', False)
                    link.error = result.get('error')
                    link.response_time = result.get('response_time', 0.0)
                    results.append(link)
                
                # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
                await asyncio.sleep(0.01)
            return results
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –¥–ª—è –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        domain_tasks = [process_domain(domain, items) for domain, items in domain_groups.items()]
        domain_results = await asyncio.gather(*domain_tasks)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        all_results = [None] * len(links)
        for domain_result in domain_results:
            for link in domain_result:
                # –ù–∞—Ö–æ–¥–∏–º –∏—Å—Ö–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å (–≥—Ä—É–±—ã–π –º–µ—Ç–æ–¥, –Ω–æ –±—ã—Å—Ç—Ä—ã–π)
                for i, orig_link in enumerate(links):
                    if orig_link.url == link.url and all_results[i] is None:
                        all_results[i] = link
                        break
        
        return [link for link in all_results if link is not None]

def report_results(links: List[LinkInfo]):
    """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    valid = [l for l in links if l.is_valid]
    invalid = [l for l in links if not l.is_valid]
    
    print(f"\n{'='*60}")
    print(f"‚úÖ –í–ê–õ–ò–î–ù–´–ï –°–°–´–õ–ö–ò (200-399): {len(valid)}")
    print(f"{'-'*60}")
    for i, link in enumerate(valid[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
        print(f"{i}. {link.text[:50]}")
        print(f"   ‚Üí {link.url[:70]}")
        print(f"   üìä {link.status_code} | ‚è±Ô∏è {link.response_time:.3f}—Å\n")
    
    if len(valid) > 10:
        print(f"... –∏ –µ—â–µ {len(valid) - 10} –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
    
    print(f"\n{'='*60}")
    print(f"‚ùå –ù–ï–í–ê–õ–ò–î–ù–´–ï –°–°–´–õ–ö–ò: {len(invalid)}")
    print(f"{'-'*60}")
    for i, link in enumerate(invalid[:10], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
        status = link.status_code if link.status_code else "ERR"
        error = f" | {link.error}" if link.error else ""
        print(f"{i}. {link.text[:50]}")
        print(f"   ‚Üí {link.url[:70]}")
        print(f"   üìä {status}{error}")
        print(f"   ‚è±Ô∏è {link.response_time:.3f}—Å\n")
    
    if len(invalid) > 10:
        print(f"... –∏ –µ—â–µ {len(invalid) - 10} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    with open('valid_links.txt', 'w', encoding='utf-8') as f:
        for link in valid:
            f.write(f"{link.text} | {link.url} | {link.status_code} | {link.response_time:.3f}\n")
    
    with open('invalid_links.txt', 'w', encoding='utf-8') as f:
        for link in invalid:
            status = link.status_code or "ERR"
            error = link.error or ""
            f.write(f"{link.text} | {link.url} | {status} | {error} | {link.response_time:.3f}\n")
    
    print(f"\n‚úÖ –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:")
    print(f"   - valid_links.txt ({len(valid)} —Å—Å—ã–ª–æ–∫)")
    print(f"   - invalid_links.txt ({len(invalid)} —Å—Å—ã–ª–æ–∫)")

async def main(url: str):
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    signal.signal(signal.SIGINT, lambda s, f: sys.exit(0))
    
    print("="*60)
    print("üöÄ –°–í–ï–†–•–ë–´–°–¢–†–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–°–´–õ–û–ö –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú SELENIUM")
    print("="*60)
    print(f"üéØ –¶–µ–ª–µ–≤–æ–π URL: {url}")
    print(f"‚ö° –°—Ç—Ä–∞—Ç–µ–≥–∏—è: Selenium –¥–ª—è —Å–±–æ—Ä–∞ + HEAD –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
    print("-"*60)
    
    total_start = time.time()
    
    # –≠—Ç–∞–ø 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —á–µ—Ä–µ–∑ Selenium
    links = get_links_with_selenium(url)
    if not links:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        return
    
    # –≠—Ç–∞–ø 2: –°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤
    print(f"\n‚ö° –ó–∞–ø—É—Å–∫ —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ {len(links)} —Å—Å—ã–ª–æ–∫...")
    check_start = time.time()
    
    checked_links = await check_links_ultra_fast(links)
    
    check_time = time.time() - check_start
    total_time = time.time() - total_start
    
    # –≠—Ç–∞–ø 3: –û—Ç—á–µ—Ç
    print(f"\n{'='*60}")
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {check_time:.2f} —Å–µ–∫")
    print(f"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {len(links)/check_time:.1f} —Å—Å—ã–ª–æ–∫/—Å–µ–∫")
    print(f"üèÅ –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
    
    report_results(checked_links)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–°–≤–µ—Ä—Ö–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium')
    parser.add_argument('url', type=str, help='URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    args = parser.parse_args()
    
    asyncio.run(main(args.url))